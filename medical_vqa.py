import streamlit as st
from PIL import Image
import torch
from transformers import (
    BlipProcessor,
    BlipForQuestionAnswering,
    MarianMTModel,
    MarianTokenizer,
)

# ---------------------------------------------------------
# 1) Load every heavy resource exactly **once**
# ---------------------------------------------------------
@st.cache_resource(show_spinner=False)
def load_nlp_models():
    # BLIP VQA
    blip_model = BlipForQuestionAnswering.from_pretrained("sharawy53/diploma")
    blip_processor = BlipProcessor.from_pretrained("sharawy53/diploma")

    # Translation models
    ar_en_tok = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ar-en")
    ar_en_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-ar-en")

    en_ar_tok = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-ar")
    en_ar_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-ar")

    return {
        "blip_model": blip_model,
        "blip_processor": blip_processor,
        "ar_en_tok": ar_en_tok,
        "ar_en_model": ar_en_model,
        "en_ar_tok": en_ar_tok,
        "en_ar_model": en_ar_model,
    }


models = load_nlp_models()

# ---------------------------------------------------------
# 2) Helper: translation
# ---------------------------------------------------------
def translate_ar_to_en(text: str) -> str:
    tkn = models["ar_en_tok"]
    mdl = models["ar_en_model"]
    inputs = tkn(text, return_tensors="pt", padding=True, truncation=True)
    out = mdl.generate(**inputs)
    return tkn.decode(out[0], skip_special_tokens=True).strip()


def translate_en_to_ar(text: str) -> str:
    tkn = models["en_ar_tok"]
    mdl = models["en_ar_model"]
    inputs = tkn(text, return_tensors="pt", padding=True, truncation=True)
    out = mdl.generate(**inputs)
    return tkn.decode(out[0], skip_special_tokens=True).strip()


# â€œSmartâ€ manual medical dictionary
MED_DICT = {
    "chest x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ© Ù„Ù„ØµØ¯Ø±",
    "x-ray": "Ø£Ø´Ø¹Ø© Ø³ÙŠÙ†ÙŠØ©",
    "ct scan": "ØªØµÙˆÙŠØ± Ù…Ù‚Ø·Ø¹ÙŠ Ù…Ø­ÙˆØ³Ø¨",
    "mri": "ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ",
    "ultrasound": "ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ù…ÙˆØ¬Ø§Øª ÙÙˆÙ‚ Ø§Ù„ØµÙˆØªÙŠØ©",
    "normal": "Ø·Ø¨ÙŠØ¹ÙŠ",
    "abnormal": "ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ",
    "brain": "Ø§Ù„Ø¯Ù…Ø§Øº",
    "fracture": "ÙƒØ³Ø±",
    "no abnormality detected": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø´Ø°ÙˆØ°Ø§Øª",
    "left lung": "Ø§Ù„Ø±Ø¦Ø© Ø§Ù„ÙŠØ³Ø±Ù‰",
    "right lung": "Ø§Ù„Ø±Ø¦Ø© Ø§Ù„ÙŠÙ…Ù†Ù‰",
}


def smart_translate_answer(answer_en: str) -> str:
    key = answer_en.lower().strip()
    return MED_DICT.get(key, translate_en_to_ar(answer_en))


# ---------------------------------------------------------
# 3) Core VQA pipeline
# ---------------------------------------------------------
def vqa_multilingual(image: Image.Image, question: str):
    if image is None or question.strip() == "":
        return "", "", "", ""

    # Detect script
    is_arabic = any("\u0600" <= c <= "\u06FF" for c in question)

    if is_arabic:
        q_ar = question.strip()
        q_en = translate_ar_to_en(q_ar)
    else:
        q_en = question.strip()
        q_ar = translate_en_to_ar(q_en)

    proc = models["blip_processor"]
    blip = models["blip_model"]

    inputs = proc(image, q_en, return_tensors="pt")
    with torch.no_grad():
        output = blip.generate(**inputs)
    ans_en = proc.decode(output[0], skip_special_tokens=True).strip()
    ans_ar = smart_translate_answer(ans_en)

    return q_ar, q_en, ans_ar, ans_en


# ---------------------------------------------------------
# 4) Streamlit UI
# ---------------------------------------------------------
st.set_page_config(
    page_title="Ù†Ù…ÙˆØ°Ø¬ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© Ù„Ù„Ø£Ø´Ø¹Ø© (CT/X-Ray/MRI)",
    layout="wide",
)

st.title("ðŸ©» Ù†Ù…ÙˆØ°Ø¬ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© (Ø¹Ø±Ø¨ÙŠ â€“ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ) Ù„Ø£Ø³Ø¦Ù„Ø© ØµÙˆØ± Ø§Ù„Ø£Ø´Ø¹Ø©")
st.markdown("Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø·Ø¨ÙŠØ© ÙˆØ§Ø³Ø£Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ "
            "ÙˆØ³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØªÙŠÙ†.")

col_left, col_right = st.columns([1, 2])

with col_left:
    uploaded = st.file_uploader("ðŸ” Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© Ø§Ù„Ø£Ø´Ø¹Ø©", type=["jpg", "jpeg", "png"])
    if uploaded:
        img = Image.open(uploaded).convert("RGB")
        st.image(img, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", use_column_width=True)
    else:
        img = None

with col_right:
    question_txt = st.text_input("ðŸ’¬ Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ (Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)")

    if st.button("Ø¥Ø±Ø³Ø§Ù„"):
        q_ar, q_en, a_ar, a_en = vqa_multilingual(img, question_txt)

        st.text_area("ðŸŸ  Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", q_ar, height=60)
        st.text_area("ðŸŸ¢ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©", q_en, height=60)
        st.text_area("ðŸŸ  Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", a_ar, height=60)
        st.text_area("ðŸŸ¢ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©", a_en, height=60)
